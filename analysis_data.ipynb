{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_qa_net\n",
    "import loader\n",
    "config = config_qa_net.config\n",
    "embedding_np = loader.load_w2v(config.embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param ={\n",
    "    'dropout_p':0.1,\n",
    "    'encoder_dropout_p':0.1,\n",
    "    'embedding': embedding_np,\n",
    "    'hidden_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fill_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6e08b51f1ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ff9e305dfd79>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighway_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHighway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthwiseSeparableConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthwiseSeparableConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ff9e305dfd79>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_ch, out_ch, k)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mconstant_\u001b[0;34m(tensor, val)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fill_'"
     ]
    }
   ],
   "source": [
    "a = Model(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): ExtendEmbedding(\n",
       "    (sd_embedding): Embedding(\n",
       "      (embedding): Embedding(113315, 256, padding_idx=0)\n",
       "    )\n",
       "    (tag_embedding): Embedding(60, 4, padding_idx=0)\n",
       "  )\n",
       "  (content_conv): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(262, 262, kernel_size=(5,), stride=(1,), padding=(2,), groups=262)\n",
       "    (pointwise_conv): Conv1d(262, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (question_conv): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(260, 260, kernel_size=(5,), stride=(1,), padding=(2,), groups=260)\n",
       "    (pointwise_conv): Conv1d(260, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (c_enc): EncoderBlock(\n",
       "    (convs): ModuleList(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (self_att): SelfAttention(\n",
       "      (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (Wqs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wks): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wvs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (pos): PosEncoder()\n",
       "    (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_2): ModuleList(\n",
       "      (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (q_enc): EncoderBlock(\n",
       "    (convs): ModuleList(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (self_att): SelfAttention(\n",
       "      (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (Wqs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wks): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wvs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (pos): PosEncoder()\n",
       "    (norm_1): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_2): ModuleList(\n",
       "      (0): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (1): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (3): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (norm_3): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cq_att): CQAttention(\n",
       "    (w): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       "  (cq_resizer): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), groups=512)\n",
       "    (pointwise_conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (model_enc_blks): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pointer): Pointer(\n",
       "    (w1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (w2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.sd_embedding.embedding.weight 29008640\n",
      "embedding.tag_embedding.weight 240\n",
      "content_conv.depthwise_conv.weight 1310\n",
      "content_conv.depthwise_conv.bias 262\n",
      "content_conv.pointwise_conv.weight 33536\n",
      "content_conv.pointwise_conv.bias 128\n",
      "question_conv.depthwise_conv.weight 1300\n",
      "question_conv.depthwise_conv.bias 260\n",
      "question_conv.pointwise_conv.weight 33280\n",
      "question_conv.pointwise_conv.bias 128\n",
      "c_enc.convs.0.depthwise_conv.weight 896\n",
      "c_enc.convs.0.depthwise_conv.bias 128\n",
      "c_enc.convs.0.pointwise_conv.weight 16384\n",
      "c_enc.convs.0.pointwise_conv.bias 128\n",
      "c_enc.convs.1.depthwise_conv.weight 896\n",
      "c_enc.convs.1.depthwise_conv.bias 128\n",
      "c_enc.convs.1.pointwise_conv.weight 16384\n",
      "c_enc.convs.1.pointwise_conv.bias 128\n",
      "c_enc.convs.2.depthwise_conv.weight 896\n",
      "c_enc.convs.2.depthwise_conv.bias 128\n",
      "c_enc.convs.2.pointwise_conv.weight 16384\n",
      "c_enc.convs.2.pointwise_conv.bias 128\n",
      "c_enc.convs.3.depthwise_conv.weight 896\n",
      "c_enc.convs.3.depthwise_conv.bias 128\n",
      "c_enc.convs.3.pointwise_conv.weight 16384\n",
      "c_enc.convs.3.pointwise_conv.bias 128\n",
      "c_enc.self_att.W0.weight 16384\n",
      "c_enc.self_att.W0.bias 128\n",
      "c_enc.self_att.Wqs.0.weight 2048\n",
      "c_enc.self_att.Wqs.0.bias 16\n",
      "c_enc.self_att.Wqs.1.weight 2048\n",
      "c_enc.self_att.Wqs.1.bias 16\n",
      "c_enc.self_att.Wqs.2.weight 2048\n",
      "c_enc.self_att.Wqs.2.bias 16\n",
      "c_enc.self_att.Wqs.3.weight 2048\n",
      "c_enc.self_att.Wqs.3.bias 16\n",
      "c_enc.self_att.Wqs.4.weight 2048\n",
      "c_enc.self_att.Wqs.4.bias 16\n",
      "c_enc.self_att.Wqs.5.weight 2048\n",
      "c_enc.self_att.Wqs.5.bias 16\n",
      "c_enc.self_att.Wqs.6.weight 2048\n",
      "c_enc.self_att.Wqs.6.bias 16\n",
      "c_enc.self_att.Wqs.7.weight 2048\n",
      "c_enc.self_att.Wqs.7.bias 16\n",
      "c_enc.self_att.Wks.0.weight 2048\n",
      "c_enc.self_att.Wks.0.bias 16\n",
      "c_enc.self_att.Wks.1.weight 2048\n",
      "c_enc.self_att.Wks.1.bias 16\n",
      "c_enc.self_att.Wks.2.weight 2048\n",
      "c_enc.self_att.Wks.2.bias 16\n",
      "c_enc.self_att.Wks.3.weight 2048\n",
      "c_enc.self_att.Wks.3.bias 16\n",
      "c_enc.self_att.Wks.4.weight 2048\n",
      "c_enc.self_att.Wks.4.bias 16\n",
      "c_enc.self_att.Wks.5.weight 2048\n",
      "c_enc.self_att.Wks.5.bias 16\n",
      "c_enc.self_att.Wks.6.weight 2048\n",
      "c_enc.self_att.Wks.6.bias 16\n",
      "c_enc.self_att.Wks.7.weight 2048\n",
      "c_enc.self_att.Wks.7.bias 16\n",
      "c_enc.self_att.Wvs.0.weight 2048\n",
      "c_enc.self_att.Wvs.0.bias 16\n",
      "c_enc.self_att.Wvs.1.weight 2048\n",
      "c_enc.self_att.Wvs.1.bias 16\n",
      "c_enc.self_att.Wvs.2.weight 2048\n",
      "c_enc.self_att.Wvs.2.bias 16\n",
      "c_enc.self_att.Wvs.3.weight 2048\n",
      "c_enc.self_att.Wvs.3.bias 16\n",
      "c_enc.self_att.Wvs.4.weight 2048\n",
      "c_enc.self_att.Wvs.4.bias 16\n",
      "c_enc.self_att.Wvs.5.weight 2048\n",
      "c_enc.self_att.Wvs.5.bias 16\n",
      "c_enc.self_att.Wvs.6.weight 2048\n",
      "c_enc.self_att.Wvs.6.bias 16\n",
      "c_enc.self_att.Wvs.7.weight 2048\n",
      "c_enc.self_att.Wvs.7.bias 16\n",
      "c_enc.fc.weight 16384\n",
      "c_enc.fc.bias 128\n",
      "c_enc.norm_1.weight 64000\n",
      "c_enc.norm_1.bias 64000\n",
      "c_enc.norm_2.0.weight 64000\n",
      "c_enc.norm_2.0.bias 64000\n",
      "c_enc.norm_2.1.weight 64000\n",
      "c_enc.norm_2.1.bias 64000\n",
      "c_enc.norm_2.2.weight 64000\n",
      "c_enc.norm_2.2.bias 64000\n",
      "c_enc.norm_2.3.weight 64000\n",
      "c_enc.norm_2.3.bias 64000\n",
      "c_enc.norm_3.weight 64000\n",
      "c_enc.norm_3.bias 64000\n",
      "q_enc.convs.0.depthwise_conv.weight 896\n",
      "q_enc.convs.0.depthwise_conv.bias 128\n",
      "q_enc.convs.0.pointwise_conv.weight 16384\n",
      "q_enc.convs.0.pointwise_conv.bias 128\n",
      "q_enc.convs.1.depthwise_conv.weight 896\n",
      "q_enc.convs.1.depthwise_conv.bias 128\n",
      "q_enc.convs.1.pointwise_conv.weight 16384\n",
      "q_enc.convs.1.pointwise_conv.bias 128\n",
      "q_enc.convs.2.depthwise_conv.weight 896\n",
      "q_enc.convs.2.depthwise_conv.bias 128\n",
      "q_enc.convs.2.pointwise_conv.weight 16384\n",
      "q_enc.convs.2.pointwise_conv.bias 128\n",
      "q_enc.convs.3.depthwise_conv.weight 896\n",
      "q_enc.convs.3.depthwise_conv.bias 128\n",
      "q_enc.convs.3.pointwise_conv.weight 16384\n",
      "q_enc.convs.3.pointwise_conv.bias 128\n",
      "q_enc.self_att.W0.weight 16384\n",
      "q_enc.self_att.W0.bias 128\n",
      "q_enc.self_att.Wqs.0.weight 2048\n",
      "q_enc.self_att.Wqs.0.bias 16\n",
      "q_enc.self_att.Wqs.1.weight 2048\n",
      "q_enc.self_att.Wqs.1.bias 16\n",
      "q_enc.self_att.Wqs.2.weight 2048\n",
      "q_enc.self_att.Wqs.2.bias 16\n",
      "q_enc.self_att.Wqs.3.weight 2048\n",
      "q_enc.self_att.Wqs.3.bias 16\n",
      "q_enc.self_att.Wqs.4.weight 2048\n",
      "q_enc.self_att.Wqs.4.bias 16\n",
      "q_enc.self_att.Wqs.5.weight 2048\n",
      "q_enc.self_att.Wqs.5.bias 16\n",
      "q_enc.self_att.Wqs.6.weight 2048\n",
      "q_enc.self_att.Wqs.6.bias 16\n",
      "q_enc.self_att.Wqs.7.weight 2048\n",
      "q_enc.self_att.Wqs.7.bias 16\n",
      "q_enc.self_att.Wks.0.weight 2048\n",
      "q_enc.self_att.Wks.0.bias 16\n",
      "q_enc.self_att.Wks.1.weight 2048\n",
      "q_enc.self_att.Wks.1.bias 16\n",
      "q_enc.self_att.Wks.2.weight 2048\n",
      "q_enc.self_att.Wks.2.bias 16\n",
      "q_enc.self_att.Wks.3.weight 2048\n",
      "q_enc.self_att.Wks.3.bias 16\n",
      "q_enc.self_att.Wks.4.weight 2048\n",
      "q_enc.self_att.Wks.4.bias 16\n",
      "q_enc.self_att.Wks.5.weight 2048\n",
      "q_enc.self_att.Wks.5.bias 16\n",
      "q_enc.self_att.Wks.6.weight 2048\n",
      "q_enc.self_att.Wks.6.bias 16\n",
      "q_enc.self_att.Wks.7.weight 2048\n",
      "q_enc.self_att.Wks.7.bias 16\n",
      "q_enc.self_att.Wvs.0.weight 2048\n",
      "q_enc.self_att.Wvs.0.bias 16\n",
      "q_enc.self_att.Wvs.1.weight 2048\n",
      "q_enc.self_att.Wvs.1.bias 16\n",
      "q_enc.self_att.Wvs.2.weight 2048\n",
      "q_enc.self_att.Wvs.2.bias 16\n",
      "q_enc.self_att.Wvs.3.weight 2048\n",
      "q_enc.self_att.Wvs.3.bias 16\n",
      "q_enc.self_att.Wvs.4.weight 2048\n",
      "q_enc.self_att.Wvs.4.bias 16\n",
      "q_enc.self_att.Wvs.5.weight 2048\n",
      "q_enc.self_att.Wvs.5.bias 16\n",
      "q_enc.self_att.Wvs.6.weight 2048\n",
      "q_enc.self_att.Wvs.6.bias 16\n",
      "q_enc.self_att.Wvs.7.weight 2048\n",
      "q_enc.self_att.Wvs.7.bias 16\n",
      "q_enc.fc.weight 16384\n",
      "q_enc.fc.bias 128\n",
      "q_enc.norm_1.weight 19200\n",
      "q_enc.norm_1.bias 19200\n",
      "q_enc.norm_2.0.weight 19200\n",
      "q_enc.norm_2.0.bias 19200\n",
      "q_enc.norm_2.1.weight 19200\n",
      "q_enc.norm_2.1.bias 19200\n",
      "q_enc.norm_2.2.weight 19200\n",
      "q_enc.norm_2.2.bias 19200\n",
      "q_enc.norm_2.3.weight 19200\n",
      "q_enc.norm_2.3.bias 19200\n",
      "q_enc.norm_3.weight 19200\n",
      "q_enc.norm_3.bias 19200\n",
      "cq_att.w.weight 384\n",
      "cq_att.w.bias 1\n",
      "cq_resizer.depthwise_conv.weight 2560\n",
      "cq_resizer.depthwise_conv.bias 512\n",
      "cq_resizer.pointwise_conv.weight 65536\n",
      "cq_resizer.pointwise_conv.bias 128\n",
      "model_enc_blks.0.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.0.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.0.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.0.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.0.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.0.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.0.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.0.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.0.self_att.W0.weight 16384\n",
      "model_enc_blks.0.self_att.W0.bias 128\n",
      "model_enc_blks.0.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.0.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.0.bias 16\n",
      "model_enc_blks.0.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.1.bias 16\n",
      "model_enc_blks.0.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.2.bias 16\n",
      "model_enc_blks.0.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.3.bias 16\n",
      "model_enc_blks.0.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.4.bias 16\n",
      "model_enc_blks.0.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.5.bias 16\n",
      "model_enc_blks.0.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.6.bias 16\n",
      "model_enc_blks.0.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.7.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.0.fc.weight 16384\n",
      "model_enc_blks.0.fc.bias 128\n",
      "model_enc_blks.0.norm_1.weight 64000\n",
      "model_enc_blks.0.norm_1.bias 64000\n",
      "model_enc_blks.0.norm_2.0.weight 64000\n",
      "model_enc_blks.0.norm_2.0.bias 64000\n",
      "model_enc_blks.0.norm_2.1.weight 64000\n",
      "model_enc_blks.0.norm_2.1.bias 64000\n",
      "model_enc_blks.0.norm_3.weight 64000\n",
      "model_enc_blks.0.norm_3.bias 64000\n",
      "model_enc_blks.1.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.1.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.1.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.1.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.1.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.1.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.1.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.1.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.1.self_att.W0.weight 16384\n",
      "model_enc_blks.1.self_att.W0.bias 128\n",
      "model_enc_blks.1.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.1.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.0.bias 16\n",
      "model_enc_blks.1.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.1.bias 16\n",
      "model_enc_blks.1.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.2.bias 16\n",
      "model_enc_blks.1.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.3.bias 16\n",
      "model_enc_blks.1.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.4.bias 16\n",
      "model_enc_blks.1.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.5.bias 16\n",
      "model_enc_blks.1.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.6.bias 16\n",
      "model_enc_blks.1.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.7.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.1.fc.weight 16384\n",
      "model_enc_blks.1.fc.bias 128\n",
      "model_enc_blks.1.norm_1.weight 64000\n",
      "model_enc_blks.1.norm_1.bias 64000\n",
      "model_enc_blks.1.norm_2.0.weight 64000\n",
      "model_enc_blks.1.norm_2.0.bias 64000\n",
      "model_enc_blks.1.norm_2.1.weight 64000\n",
      "model_enc_blks.1.norm_2.1.bias 64000\n",
      "model_enc_blks.1.norm_3.weight 64000\n",
      "model_enc_blks.1.norm_3.bias 64000\n",
      "model_enc_blks.2.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.2.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.2.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.2.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.2.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.2.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.2.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.2.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.2.self_att.W0.weight 16384\n",
      "model_enc_blks.2.self_att.W0.bias 128\n",
      "model_enc_blks.2.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.2.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.0.bias 16\n",
      "model_enc_blks.2.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.1.bias 16\n",
      "model_enc_blks.2.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.2.bias 16\n",
      "model_enc_blks.2.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.3.bias 16\n",
      "model_enc_blks.2.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.4.bias 16\n",
      "model_enc_blks.2.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.5.bias 16\n",
      "model_enc_blks.2.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.6.bias 16\n",
      "model_enc_blks.2.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.7.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.2.fc.weight 16384\n",
      "model_enc_blks.2.fc.bias 128\n",
      "model_enc_blks.2.norm_1.weight 64000\n",
      "model_enc_blks.2.norm_1.bias 64000\n",
      "model_enc_blks.2.norm_2.0.weight 64000\n",
      "model_enc_blks.2.norm_2.0.bias 64000\n",
      "model_enc_blks.2.norm_2.1.weight 64000\n",
      "model_enc_blks.2.norm_2.1.bias 64000\n",
      "model_enc_blks.2.norm_3.weight 64000\n",
      "model_enc_blks.2.norm_3.bias 64000\n",
      "model_enc_blks.3.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.3.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.3.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.3.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.3.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.3.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.3.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.3.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.3.self_att.W0.weight 16384\n",
      "model_enc_blks.3.self_att.W0.bias 128\n",
      "model_enc_blks.3.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.3.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.0.bias 16\n",
      "model_enc_blks.3.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.1.bias 16\n",
      "model_enc_blks.3.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.2.bias 16\n",
      "model_enc_blks.3.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.3.bias 16\n",
      "model_enc_blks.3.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.4.bias 16\n",
      "model_enc_blks.3.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.5.bias 16\n",
      "model_enc_blks.3.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.6.bias 16\n",
      "model_enc_blks.3.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.7.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.3.fc.weight 16384\n",
      "model_enc_blks.3.fc.bias 128\n",
      "model_enc_blks.3.norm_1.weight 64000\n",
      "model_enc_blks.3.norm_1.bias 64000\n",
      "model_enc_blks.3.norm_2.0.weight 64000\n",
      "model_enc_blks.3.norm_2.0.bias 64000\n",
      "model_enc_blks.3.norm_2.1.weight 64000\n",
      "model_enc_blks.3.norm_2.1.bias 64000\n",
      "model_enc_blks.3.norm_3.weight 64000\n",
      "model_enc_blks.3.norm_3.bias 64000\n",
      "model_enc_blks.4.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.4.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.4.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.4.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.4.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.4.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.4.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.4.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.4.self_att.W0.weight 16384\n",
      "model_enc_blks.4.self_att.W0.bias 128\n",
      "model_enc_blks.4.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.4.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.0.bias 16\n",
      "model_enc_blks.4.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.1.bias 16\n",
      "model_enc_blks.4.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.2.bias 16\n",
      "model_enc_blks.4.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.3.bias 16\n",
      "model_enc_blks.4.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.4.bias 16\n",
      "model_enc_blks.4.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.5.bias 16\n",
      "model_enc_blks.4.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.6.bias 16\n",
      "model_enc_blks.4.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.7.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.4.fc.weight 16384\n",
      "model_enc_blks.4.fc.bias 128\n",
      "model_enc_blks.4.norm_1.weight 64000\n",
      "model_enc_blks.4.norm_1.bias 64000\n",
      "model_enc_blks.4.norm_2.0.weight 64000\n",
      "model_enc_blks.4.norm_2.0.bias 64000\n",
      "model_enc_blks.4.norm_2.1.weight 64000\n",
      "model_enc_blks.4.norm_2.1.bias 64000\n",
      "model_enc_blks.4.norm_3.weight 64000\n",
      "model_enc_blks.4.norm_3.bias 64000\n",
      "model_enc_blks.5.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.5.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.5.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.5.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.5.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.5.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.5.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.5.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.5.self_att.W0.weight 16384\n",
      "model_enc_blks.5.self_att.W0.bias 128\n",
      "model_enc_blks.5.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.5.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.0.bias 16\n",
      "model_enc_blks.5.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.1.bias 16\n",
      "model_enc_blks.5.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.2.bias 16\n",
      "model_enc_blks.5.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.3.bias 16\n",
      "model_enc_blks.5.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.4.bias 16\n",
      "model_enc_blks.5.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.5.bias 16\n",
      "model_enc_blks.5.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.6.bias 16\n",
      "model_enc_blks.5.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.7.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.5.fc.weight 16384\n",
      "model_enc_blks.5.fc.bias 128\n",
      "model_enc_blks.5.norm_1.weight 64000\n",
      "model_enc_blks.5.norm_1.bias 64000\n",
      "model_enc_blks.5.norm_2.0.weight 64000\n",
      "model_enc_blks.5.norm_2.0.bias 64000\n",
      "model_enc_blks.5.norm_2.1.weight 64000\n",
      "model_enc_blks.5.norm_2.1.bias 64000\n",
      "model_enc_blks.5.norm_3.weight 64000\n",
      "model_enc_blks.5.norm_3.bias 64000\n",
      "model_enc_blks.6.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.6.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.6.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.6.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.6.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.6.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.6.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.6.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.6.self_att.W0.weight 16384\n",
      "model_enc_blks.6.self_att.W0.bias 128\n",
      "model_enc_blks.6.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.6.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.0.bias 16\n",
      "model_enc_blks.6.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.1.bias 16\n",
      "model_enc_blks.6.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.2.bias 16\n",
      "model_enc_blks.6.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.3.bias 16\n",
      "model_enc_blks.6.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.4.bias 16\n",
      "model_enc_blks.6.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.5.bias 16\n",
      "model_enc_blks.6.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.6.bias 16\n",
      "model_enc_blks.6.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.7.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.6.fc.weight 16384\n",
      "model_enc_blks.6.fc.bias 128\n",
      "model_enc_blks.6.norm_1.weight 64000\n",
      "model_enc_blks.6.norm_1.bias 64000\n",
      "model_enc_blks.6.norm_2.0.weight 64000\n",
      "model_enc_blks.6.norm_2.0.bias 64000\n",
      "model_enc_blks.6.norm_2.1.weight 64000\n",
      "model_enc_blks.6.norm_2.1.bias 64000\n",
      "model_enc_blks.6.norm_3.weight 64000\n",
      "model_enc_blks.6.norm_3.bias 64000\n",
      "pointer.w1.weight 256\n",
      "pointer.w1.bias 1\n",
      "pointer.w2.weight 256\n",
      "pointer.w2.bias 1\n"
     ]
    }
   ],
   "source": [
    "for name, p in a.named_parameters():\n",
    "    print(name, p.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
