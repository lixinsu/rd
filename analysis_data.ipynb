{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding = utf-8\n",
    "# author = xy\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as f\n",
    "from modules.layers import embedding\n",
    "import utils\n",
    "\n",
    "Max_Content_len = 500\n",
    "Max_Question_len = 150\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\" qa-net for reading comprehension\"\"\"\n",
    "    def __init__(self, param):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.dropout_p = param['dropout_p']\n",
    "        self.encoder_dropout_p = param['encoder_dropout_p']\n",
    "        self.w2i = param['embedding'].shape[1]\n",
    "        self.hidden_size = param['hidden_size']\n",
    "\n",
    "        self.embedding = embedding.ExtendEmbedding(param['embedding'])\n",
    "        self.flag = False\n",
    "        if self.flag:\n",
    "            self.highway_c = Highway(self.w2i+6)\n",
    "            self.highway_q = Highway(self.w2i+4)\n",
    "\n",
    "        self.content_conv = DepthwiseSeparableConv(self.w2i+6, self.hidden_size, 5)\n",
    "        self.question_conv = DepthwiseSeparableConv(self.w2i+4, self.hidden_size, 5)\n",
    "\n",
    "        self.c_enc = EncoderBlock(conv_num=2, d=self.hidden_size, k=7, length=Max_Content_len, dropout_p=self.dropout_p)\n",
    "        self.q_enc = EncoderBlock(conv_num=2, d=self.hidden_size, k=7, length=Max_Question_len, dropout_p=self.dropout_p)\n",
    "\n",
    "        self.cq_att = CQAttention(self.hidden_size, self.dropout_p)\n",
    "\n",
    "        self.cq_resizer = DepthwiseSeparableConv(self.hidden_size*4, self.hidden_size, 5)\n",
    "\n",
    "        self.model_enc_blks = nn.ModuleList([EncoderBlock(conv_num=2, d=self.hidden_size, k=5, length=Max_Content_len,\n",
    "                                            dropout_p=self.dropout_p) for _ in range(2)])\n",
    "\n",
    "        self.pointer = Pointer(self.hidden_size)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        :param batch:\n",
    "        :return: (2, batch_size, c_len)\n",
    "        \"\"\"\n",
    "\n",
    "        content = batch[0: 4]\n",
    "        question = batch[4: 6]\n",
    "\n",
    "        # mask\n",
    "        content_mask = utils.get_mask(content[0])\n",
    "        question_mask = utils.get_mask(question[0])\n",
    "\n",
    "        # embedding\n",
    "        content_vec = self.embedding(content, True)  # (c_len, batch_size, w2i_size+6)\n",
    "        question_vec = self.embedding(question, False)  # (q_len, batch_size, w2i_size+4)\n",
    "\n",
    "        # embedding done\n",
    "        if self.flag:\n",
    "            content_vec = f.dropout(content_vec, p=self.encoder_dropout_p, training=self.training)\n",
    "            question_vec = f.dropout(question_vec, p=self.encoder_dropout_p, training=self.training)\n",
    "            content_vec = self.highway_c(content_vec)\n",
    "            question_vec = self.highway_q(question_vec)  # (q_len, batch_size, w2i_size+4)\n",
    "\n",
    "        # conv\n",
    "        content_vec = content_vec.transpose(0, 1).transpose(1, 2)  # (batch_size, w2i_size+6, c_len)\n",
    "        question_vec = question_vec.transpose(0, 1).transpose(1, 2)  # (batch_size, w2i_size+4, q_len)\n",
    "        content_vec = self.content_conv(content_vec)  # (batch_size, hidden_size, c_len)\n",
    "        question_vec = self.question_conv(question_vec)  # (batch_size, hidden_size, q_len)\n",
    "\n",
    "        # encoder\n",
    "        Ce = self.c_enc(content_vec, content_mask)\n",
    "        Qe = self.q_enc(question_vec, question_mask)\n",
    "\n",
    "        # cq attention\n",
    "        X = self.cq_att(Ce, content_mask, Qe, question_mask)  # (batch_size, d*4, c_len)\n",
    "\n",
    "        # model encoder layer\n",
    "        M0 = self.cq_resizer(X)  # (batch_size, d, c_len)\n",
    "        for enc in self.model_enc_blks:\n",
    "            M0 = enc(M0, content_mask)\n",
    "        M1 = M0\n",
    "        for enc in self.model_enc_blks:\n",
    "            M1 = enc(M1, content_mask)\n",
    "        M2 = M1\n",
    "        for enc in self.model_enc_blks:\n",
    "            M2 = enc(M2, content_mask)\n",
    "\n",
    "        result = self.pointer(M0, M1, M2, content_mask)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Highway, self).__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.linear = nn.ModuleList([nn.Linear(size, size) for _ in range(2)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(size, size) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (*, *, w2i_size)\n",
    "        :return: (*, *, w2i_size)\n",
    "        \"\"\"\n",
    "        for i in range(2):\n",
    "            gate = f.sigmoid(self.gate[i](x))\n",
    "            nonlinear = f.relu(self.linear[i](x))\n",
    "            x = gate * nonlinear + (1 - gate) * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "\n",
    "        self.depthwise_conv = nn.Conv1d(in_channels=in_ch, out_channels=in_ch, kernel_size=k, groups=in_ch,\n",
    "                                        padding=k // 2, bias=False)\n",
    "        self.pointwise_conv = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, kernel_size=1, padding=0)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.depthwise_conv.weight)\n",
    "        nn.init.constant_(self.depthwise_conv.bias, 0.0)\n",
    "        nn.init.kaiming_normal_(self.pointwise_conv.weight)\n",
    "        nn.init.constant_(self.pointwise_conv.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x:  (batch_size, w2i, c_len)\n",
    "        :return: (batch_size, d, c_len)\n",
    "        \"\"\"\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d, heads_num):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.heads_num = heads_num\n",
    "        self.dv = d // heads_num\n",
    "\n",
    "        self.W0 = nn.Linear(d, d)\n",
    "        self.Wqs = nn.ModuleList([torch.nn.Linear(d, self.dv) for _ in range(heads_num)])\n",
    "        self.Wks = nn.ModuleList([torch.nn.Linear(d, self.dv) for _ in range(heads_num)])\n",
    "        self.Wvs = nn.ModuleList([torch.nn.Linear(d, self.dv) for _ in range(heads_num)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        :param x: (batch_size, d, c_len)\n",
    "        :param mask: (batch_size, c_len)\n",
    "        :return: (batch_size, d, c_len)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)  # (batch_size, c_len, d)\n",
    "        h_mask = mask.unsqueeze(1)\n",
    "        v_mask = mask.unsqueeze(2)\n",
    "        heads = []\n",
    "        for i in range(self.heads_num):\n",
    "            wqs_i = self.Wqs[i](x)  # (batch_size, c_len, dv)\n",
    "            wks_i = self.Wks[i](x)\n",
    "            wvs_i = self.Wvs[i](x)\n",
    "            out = torch.bmm(wqs_i, wks_i.transpose(1, 2))  # (batch_size, c_len, c_len)\n",
    "            out = out * (1 / math.sqrt(self.dv))\n",
    "            out = utils.mask_logits(out, h_mask)\n",
    "            out = f.softmax(out, dim=2)\n",
    "            out = out * v_mask\n",
    "            head_i = torch.bmm(out, wvs_i)  # (batch_size, c_len, dv)\n",
    "            heads.append(head_i)\n",
    "        heads = torch.cat(heads, dim=2)  # (batch_size, c_len, d)\n",
    "        out = self.W0(heads).transpose(1, 2)  # (batch_size, d, c_len)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, conv_num, d, k, length, dropout_p):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_num = conv_num\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.length = length\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.convs = nn.ModuleList([DepthwiseSeparableConv(d, d, k) for _ in range(conv_num)])\n",
    "        self.self_att = SelfAttention(d, 8)\n",
    "        self.fc = nn.Linear(d, d)\n",
    "        self.pos = PosEncoder(length, d)\n",
    "        self.norm_1 = nn.LayerNorm(d)\n",
    "        self.norm_2 = nn.ModuleList([nn.LayerNorm(d) for _ in range(conv_num)])\n",
    "        self.norm_3 = nn.LayerNorm(d)\n",
    "        self.L = conv_num\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        :param x: (batch_size, d, len)\n",
    "        :param mask: (batch_size, len)\n",
    "        :return: (batch_size, d, len)\n",
    "        \"\"\"\n",
    "        out = self.pos(x)\n",
    "        res = out\n",
    "        out = self.norm_1(out.transpose(1, 2)).transpose(1, 2)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            out = conv(out)\n",
    "            out = f.relu(out)\n",
    "            out = out + res\n",
    "            if (i + 1) % 2 == 0:\n",
    "                p_drop = self.dropout_p * (i + 1) / self.L\n",
    "                out = f.dropout(out, p=p_drop, training=self.training)\n",
    "            res = out\n",
    "            out = self.norm_2[i](out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.self_att(out, mask)\n",
    "        out = out + res\n",
    "        out = f.dropout(out, p=self.dropout_p, training=self.training)\n",
    "        res = out\n",
    "        out = self.norm_3(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = self.fc(out.transpose(1, 2)).transpose(1, 2)\n",
    "        out = f.relu(out)\n",
    "        out = out + res\n",
    "        out = f.dropout(out, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PosEncoder(nn.Module):\n",
    "    def __init__(self, length, hidden_size):\n",
    "        super(PosEncoder, self).__init__()\n",
    "        freqs = torch.Tensor(\n",
    "            [10000 ** (-i / hidden_size) if i % 2 == 0 else -10000 ** ((1 - i) / hidden_size) for i in range(hidden_size)])\n",
    "        freqs = freqs.unsqueeze(1)\n",
    "        phases = torch.Tensor([0 if i % 2 == 0 else math.pi / 2 for i in range(hidden_size)])\n",
    "        phases = phases.unsqueeze(1)\n",
    "        pos = torch.arange(length).repeat(hidden_size, 1)\n",
    "        self.pos_encoding = nn.Parameter(torch.sin(pos*freqs + phases), requires_grad=False).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (batch_size, d, c_len)\n",
    "        :return: (batch_size, d, c_len)\n",
    "        \"\"\"\n",
    "        x = x + self.pos_encoding\n",
    "        return x\n",
    "\n",
    "\n",
    "class CQAttention(nn.Module):\n",
    "    def __init__(self, d, dropout_p):\n",
    "        super(CQAttention, self).__init__()\n",
    "        self.d = d\n",
    "        self.dropout_p = dropout_p\n",
    "        self. w = nn.Linear(d*3, 1)\n",
    "\n",
    "    def forward(self, c, c_mask, q, q_mask):\n",
    "        \"\"\"\n",
    "        :param c: (batch_size, d, c_len)\n",
    "        :param c_mask: (batch_size, c_len)\n",
    "        :param q: (batch_size, d, q_len)\n",
    "        :param q_mask: (batch_size, q_len)\n",
    "        :return: (batch_size, d*4, c_len)\n",
    "        \"\"\"\n",
    "        batch_size = c.size(0)\n",
    "        c_len = c.size(2)\n",
    "        q_len = q.size(2)\n",
    "\n",
    "        c = c.transpose(1, 2)  # (batch_size, c_len, d)\n",
    "        q = q.transpose(1, 2)  # (batch_size, q_len, d)\n",
    "\n",
    "        c_e = c.unsqueeze(2).expand(batch_size, c_len, q_len, self.d)\n",
    "        q_e = q.unsqueeze(1).expand(batch_size, c_len, q_len, self.d)\n",
    "        cq = c_e * q_e\n",
    "\n",
    "        s = torch.cat([q_e, c_e, cq], dim=3)\n",
    "        s = self.w(s).squeeze()  # (batch_size, c_len, q_len)\n",
    "\n",
    "        c_mask = c_mask.unsqueeze(2).expand(batch_size, c_len, q_len)\n",
    "        s1 = utils.mask_logits(s, c_mask)\n",
    "        s1 = f.softmax(s1, dim=2)\n",
    "\n",
    "        q_mask = q_mask.unsqueeze(1).expand(batch_size, c_len, q_len)\n",
    "        s2 = utils.mask_logits(s, q_mask)\n",
    "        s2 = f.softmax(s2, dim=1)\n",
    "\n",
    "        A = torch.bmm(s1, q)  # (batch_size, c_len, d)\n",
    "        B = torch.bmm(s1, s2.transpose(1, 2))  # (batch_size, c_len, c_len)\n",
    "        B = torch.bmm(B, c)  # (batch_size, c_len, d)\n",
    "\n",
    "        out = torch.cat([c, A, c*A, c*B], dim=2)  # (batch_size, c_len, d*4)\n",
    "        out = f.dropout(out, p=self.dropout_p, training=self.training)\n",
    "        out = out.transpose(1, 2)  # (batch_size, d*3, c_len)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Pointer(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.d = d\n",
    "\n",
    "        self.w1 = nn.Linear(d*2, 1)\n",
    "        self.w2 = nn.Linear(d*2, 1)\n",
    "\n",
    "    def forward(self, M0, M1, M2, mask):\n",
    "        \"\"\"\n",
    "        :param M0: (batch_size, d, c_len)\n",
    "        :param M1:\n",
    "        :param M2:\n",
    "        :param mask: (batch_size, c_len)\n",
    "        :return: (2, batch_size, c_len)\n",
    "        \"\"\"\n",
    "\n",
    "        M0 = M0.transpose(1, 2)  # (batch_size, c_len, d)\n",
    "        M1 = M1.transpose(1, 2)\n",
    "        M2 = M2.transpose(1, 2)\n",
    "\n",
    "        x1 = torch.cat([M0, M1], dim=2)\n",
    "        x2 = torch.cat([M0, M2], dim=2)\n",
    "\n",
    "        mask_1 = mask.eq(0)\n",
    "        x1 = self.w1(x1).squeeze()  # (batch_size, c_len)\n",
    "        x1.masked_fill_(mask_1, -float('inf'))\n",
    "        x1 = f.softmax(x1, dim=1)\n",
    "        x1 = x1 + x1.new_ones(x1.size()) * 1e-30\n",
    "\n",
    "        x2 = self.w2(x2).squeeze()\n",
    "        x2.masked_fill_(mask_1, -float('inf'))\n",
    "        x2 = f.softmax(x2, dim=1)\n",
    "        x2 = x2 + x2.new_ones(x2.size()) * 1e-30\n",
    "\n",
    "        result = torch.stack([x1, x2])\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_qa_net\n",
    "import loader\n",
    "config = config_qa_net.config\n",
    "embedding_np = loader.load_w2v(config.embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param ={\n",
    "    'dropout_p':0.1,\n",
    "    'encoder_dropout_p':0.1,\n",
    "    'embedding': embedding_np,\n",
    "    'hidden_size': 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fill_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6e08b51f1ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ff9e305dfd79>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighway_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHighway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthwiseSeparableConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepthwiseSeparableConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ff9e305dfd79>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_ch, out_ch, k)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepthwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointwise_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mconstant_\u001b[0;34m(tensor, val)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fill_'"
     ]
    }
   ],
   "source": [
    "a = Model(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): ExtendEmbedding(\n",
       "    (sd_embedding): Embedding(\n",
       "      (embedding): Embedding(113315, 256, padding_idx=0)\n",
       "    )\n",
       "    (tag_embedding): Embedding(60, 4, padding_idx=0)\n",
       "  )\n",
       "  (content_conv): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(262, 262, kernel_size=(5,), stride=(1,), padding=(2,), groups=262)\n",
       "    (pointwise_conv): Conv1d(262, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (question_conv): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(260, 260, kernel_size=(5,), stride=(1,), padding=(2,), groups=260)\n",
       "    (pointwise_conv): Conv1d(260, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (c_enc): EncoderBlock(\n",
       "    (convs): ModuleList(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (self_att): SelfAttention(\n",
       "      (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (Wqs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wks): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wvs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (pos): PosEncoder()\n",
       "    (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_2): ModuleList(\n",
       "      (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (q_enc): EncoderBlock(\n",
       "    (convs): ModuleList(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): DepthwiseSeparableConv(\n",
       "        (depthwise_conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128)\n",
       "        (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (self_att): SelfAttention(\n",
       "      (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (Wqs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wks): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "      (Wvs): ModuleList(\n",
       "        (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "        (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (pos): PosEncoder()\n",
       "    (norm_1): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "    (norm_2): ModuleList(\n",
       "      (0): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (1): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (2): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "      (3): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (norm_3): LayerNorm(torch.Size([128, 150]), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (cq_att): CQAttention(\n",
       "    (w): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       "  (cq_resizer): DepthwiseSeparableConv(\n",
       "    (depthwise_conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,), groups=512)\n",
       "    (pointwise_conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (model_enc_blks): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): EncoderBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (depthwise_conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=128)\n",
       "          (pointwise_conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (self_att): SelfAttention(\n",
       "        (W0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (Wqs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wks): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "        (Wvs): ModuleList(\n",
       "          (0): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (1): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (2): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (4): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (5): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (6): Linear(in_features=128, out_features=16, bias=True)\n",
       "          (7): Linear(in_features=128, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (pos): PosEncoder()\n",
       "      (norm_1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_2): ModuleList(\n",
       "        (0): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm_3): LayerNorm(torch.Size([128, 500]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pointer): Pointer(\n",
       "    (w1): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (w2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.sd_embedding.embedding.weight 29008640\n",
      "embedding.tag_embedding.weight 240\n",
      "content_conv.depthwise_conv.weight 1310\n",
      "content_conv.depthwise_conv.bias 262\n",
      "content_conv.pointwise_conv.weight 33536\n",
      "content_conv.pointwise_conv.bias 128\n",
      "question_conv.depthwise_conv.weight 1300\n",
      "question_conv.depthwise_conv.bias 260\n",
      "question_conv.pointwise_conv.weight 33280\n",
      "question_conv.pointwise_conv.bias 128\n",
      "c_enc.convs.0.depthwise_conv.weight 896\n",
      "c_enc.convs.0.depthwise_conv.bias 128\n",
      "c_enc.convs.0.pointwise_conv.weight 16384\n",
      "c_enc.convs.0.pointwise_conv.bias 128\n",
      "c_enc.convs.1.depthwise_conv.weight 896\n",
      "c_enc.convs.1.depthwise_conv.bias 128\n",
      "c_enc.convs.1.pointwise_conv.weight 16384\n",
      "c_enc.convs.1.pointwise_conv.bias 128\n",
      "c_enc.convs.2.depthwise_conv.weight 896\n",
      "c_enc.convs.2.depthwise_conv.bias 128\n",
      "c_enc.convs.2.pointwise_conv.weight 16384\n",
      "c_enc.convs.2.pointwise_conv.bias 128\n",
      "c_enc.convs.3.depthwise_conv.weight 896\n",
      "c_enc.convs.3.depthwise_conv.bias 128\n",
      "c_enc.convs.3.pointwise_conv.weight 16384\n",
      "c_enc.convs.3.pointwise_conv.bias 128\n",
      "c_enc.self_att.W0.weight 16384\n",
      "c_enc.self_att.W0.bias 128\n",
      "c_enc.self_att.Wqs.0.weight 2048\n",
      "c_enc.self_att.Wqs.0.bias 16\n",
      "c_enc.self_att.Wqs.1.weight 2048\n",
      "c_enc.self_att.Wqs.1.bias 16\n",
      "c_enc.self_att.Wqs.2.weight 2048\n",
      "c_enc.self_att.Wqs.2.bias 16\n",
      "c_enc.self_att.Wqs.3.weight 2048\n",
      "c_enc.self_att.Wqs.3.bias 16\n",
      "c_enc.self_att.Wqs.4.weight 2048\n",
      "c_enc.self_att.Wqs.4.bias 16\n",
      "c_enc.self_att.Wqs.5.weight 2048\n",
      "c_enc.self_att.Wqs.5.bias 16\n",
      "c_enc.self_att.Wqs.6.weight 2048\n",
      "c_enc.self_att.Wqs.6.bias 16\n",
      "c_enc.self_att.Wqs.7.weight 2048\n",
      "c_enc.self_att.Wqs.7.bias 16\n",
      "c_enc.self_att.Wks.0.weight 2048\n",
      "c_enc.self_att.Wks.0.bias 16\n",
      "c_enc.self_att.Wks.1.weight 2048\n",
      "c_enc.self_att.Wks.1.bias 16\n",
      "c_enc.self_att.Wks.2.weight 2048\n",
      "c_enc.self_att.Wks.2.bias 16\n",
      "c_enc.self_att.Wks.3.weight 2048\n",
      "c_enc.self_att.Wks.3.bias 16\n",
      "c_enc.self_att.Wks.4.weight 2048\n",
      "c_enc.self_att.Wks.4.bias 16\n",
      "c_enc.self_att.Wks.5.weight 2048\n",
      "c_enc.self_att.Wks.5.bias 16\n",
      "c_enc.self_att.Wks.6.weight 2048\n",
      "c_enc.self_att.Wks.6.bias 16\n",
      "c_enc.self_att.Wks.7.weight 2048\n",
      "c_enc.self_att.Wks.7.bias 16\n",
      "c_enc.self_att.Wvs.0.weight 2048\n",
      "c_enc.self_att.Wvs.0.bias 16\n",
      "c_enc.self_att.Wvs.1.weight 2048\n",
      "c_enc.self_att.Wvs.1.bias 16\n",
      "c_enc.self_att.Wvs.2.weight 2048\n",
      "c_enc.self_att.Wvs.2.bias 16\n",
      "c_enc.self_att.Wvs.3.weight 2048\n",
      "c_enc.self_att.Wvs.3.bias 16\n",
      "c_enc.self_att.Wvs.4.weight 2048\n",
      "c_enc.self_att.Wvs.4.bias 16\n",
      "c_enc.self_att.Wvs.5.weight 2048\n",
      "c_enc.self_att.Wvs.5.bias 16\n",
      "c_enc.self_att.Wvs.6.weight 2048\n",
      "c_enc.self_att.Wvs.6.bias 16\n",
      "c_enc.self_att.Wvs.7.weight 2048\n",
      "c_enc.self_att.Wvs.7.bias 16\n",
      "c_enc.fc.weight 16384\n",
      "c_enc.fc.bias 128\n",
      "c_enc.norm_1.weight 64000\n",
      "c_enc.norm_1.bias 64000\n",
      "c_enc.norm_2.0.weight 64000\n",
      "c_enc.norm_2.0.bias 64000\n",
      "c_enc.norm_2.1.weight 64000\n",
      "c_enc.norm_2.1.bias 64000\n",
      "c_enc.norm_2.2.weight 64000\n",
      "c_enc.norm_2.2.bias 64000\n",
      "c_enc.norm_2.3.weight 64000\n",
      "c_enc.norm_2.3.bias 64000\n",
      "c_enc.norm_3.weight 64000\n",
      "c_enc.norm_3.bias 64000\n",
      "q_enc.convs.0.depthwise_conv.weight 896\n",
      "q_enc.convs.0.depthwise_conv.bias 128\n",
      "q_enc.convs.0.pointwise_conv.weight 16384\n",
      "q_enc.convs.0.pointwise_conv.bias 128\n",
      "q_enc.convs.1.depthwise_conv.weight 896\n",
      "q_enc.convs.1.depthwise_conv.bias 128\n",
      "q_enc.convs.1.pointwise_conv.weight 16384\n",
      "q_enc.convs.1.pointwise_conv.bias 128\n",
      "q_enc.convs.2.depthwise_conv.weight 896\n",
      "q_enc.convs.2.depthwise_conv.bias 128\n",
      "q_enc.convs.2.pointwise_conv.weight 16384\n",
      "q_enc.convs.2.pointwise_conv.bias 128\n",
      "q_enc.convs.3.depthwise_conv.weight 896\n",
      "q_enc.convs.3.depthwise_conv.bias 128\n",
      "q_enc.convs.3.pointwise_conv.weight 16384\n",
      "q_enc.convs.3.pointwise_conv.bias 128\n",
      "q_enc.self_att.W0.weight 16384\n",
      "q_enc.self_att.W0.bias 128\n",
      "q_enc.self_att.Wqs.0.weight 2048\n",
      "q_enc.self_att.Wqs.0.bias 16\n",
      "q_enc.self_att.Wqs.1.weight 2048\n",
      "q_enc.self_att.Wqs.1.bias 16\n",
      "q_enc.self_att.Wqs.2.weight 2048\n",
      "q_enc.self_att.Wqs.2.bias 16\n",
      "q_enc.self_att.Wqs.3.weight 2048\n",
      "q_enc.self_att.Wqs.3.bias 16\n",
      "q_enc.self_att.Wqs.4.weight 2048\n",
      "q_enc.self_att.Wqs.4.bias 16\n",
      "q_enc.self_att.Wqs.5.weight 2048\n",
      "q_enc.self_att.Wqs.5.bias 16\n",
      "q_enc.self_att.Wqs.6.weight 2048\n",
      "q_enc.self_att.Wqs.6.bias 16\n",
      "q_enc.self_att.Wqs.7.weight 2048\n",
      "q_enc.self_att.Wqs.7.bias 16\n",
      "q_enc.self_att.Wks.0.weight 2048\n",
      "q_enc.self_att.Wks.0.bias 16\n",
      "q_enc.self_att.Wks.1.weight 2048\n",
      "q_enc.self_att.Wks.1.bias 16\n",
      "q_enc.self_att.Wks.2.weight 2048\n",
      "q_enc.self_att.Wks.2.bias 16\n",
      "q_enc.self_att.Wks.3.weight 2048\n",
      "q_enc.self_att.Wks.3.bias 16\n",
      "q_enc.self_att.Wks.4.weight 2048\n",
      "q_enc.self_att.Wks.4.bias 16\n",
      "q_enc.self_att.Wks.5.weight 2048\n",
      "q_enc.self_att.Wks.5.bias 16\n",
      "q_enc.self_att.Wks.6.weight 2048\n",
      "q_enc.self_att.Wks.6.bias 16\n",
      "q_enc.self_att.Wks.7.weight 2048\n",
      "q_enc.self_att.Wks.7.bias 16\n",
      "q_enc.self_att.Wvs.0.weight 2048\n",
      "q_enc.self_att.Wvs.0.bias 16\n",
      "q_enc.self_att.Wvs.1.weight 2048\n",
      "q_enc.self_att.Wvs.1.bias 16\n",
      "q_enc.self_att.Wvs.2.weight 2048\n",
      "q_enc.self_att.Wvs.2.bias 16\n",
      "q_enc.self_att.Wvs.3.weight 2048\n",
      "q_enc.self_att.Wvs.3.bias 16\n",
      "q_enc.self_att.Wvs.4.weight 2048\n",
      "q_enc.self_att.Wvs.4.bias 16\n",
      "q_enc.self_att.Wvs.5.weight 2048\n",
      "q_enc.self_att.Wvs.5.bias 16\n",
      "q_enc.self_att.Wvs.6.weight 2048\n",
      "q_enc.self_att.Wvs.6.bias 16\n",
      "q_enc.self_att.Wvs.7.weight 2048\n",
      "q_enc.self_att.Wvs.7.bias 16\n",
      "q_enc.fc.weight 16384\n",
      "q_enc.fc.bias 128\n",
      "q_enc.norm_1.weight 19200\n",
      "q_enc.norm_1.bias 19200\n",
      "q_enc.norm_2.0.weight 19200\n",
      "q_enc.norm_2.0.bias 19200\n",
      "q_enc.norm_2.1.weight 19200\n",
      "q_enc.norm_2.1.bias 19200\n",
      "q_enc.norm_2.2.weight 19200\n",
      "q_enc.norm_2.2.bias 19200\n",
      "q_enc.norm_2.3.weight 19200\n",
      "q_enc.norm_2.3.bias 19200\n",
      "q_enc.norm_3.weight 19200\n",
      "q_enc.norm_3.bias 19200\n",
      "cq_att.w.weight 384\n",
      "cq_att.w.bias 1\n",
      "cq_resizer.depthwise_conv.weight 2560\n",
      "cq_resizer.depthwise_conv.bias 512\n",
      "cq_resizer.pointwise_conv.weight 65536\n",
      "cq_resizer.pointwise_conv.bias 128\n",
      "model_enc_blks.0.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.0.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.0.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.0.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.0.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.0.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.0.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.0.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.0.self_att.W0.weight 16384\n",
      "model_enc_blks.0.self_att.W0.bias 128\n",
      "model_enc_blks.0.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.0.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.0.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.0.bias 16\n",
      "model_enc_blks.0.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.1.bias 16\n",
      "model_enc_blks.0.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.2.bias 16\n",
      "model_enc_blks.0.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.3.bias 16\n",
      "model_enc_blks.0.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.4.bias 16\n",
      "model_enc_blks.0.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.5.bias 16\n",
      "model_enc_blks.0.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.6.bias 16\n",
      "model_enc_blks.0.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wks.7.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.0.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.0.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.0.fc.weight 16384\n",
      "model_enc_blks.0.fc.bias 128\n",
      "model_enc_blks.0.norm_1.weight 64000\n",
      "model_enc_blks.0.norm_1.bias 64000\n",
      "model_enc_blks.0.norm_2.0.weight 64000\n",
      "model_enc_blks.0.norm_2.0.bias 64000\n",
      "model_enc_blks.0.norm_2.1.weight 64000\n",
      "model_enc_blks.0.norm_2.1.bias 64000\n",
      "model_enc_blks.0.norm_3.weight 64000\n",
      "model_enc_blks.0.norm_3.bias 64000\n",
      "model_enc_blks.1.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.1.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.1.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.1.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.1.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.1.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.1.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.1.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.1.self_att.W0.weight 16384\n",
      "model_enc_blks.1.self_att.W0.bias 128\n",
      "model_enc_blks.1.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.1.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.1.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.0.bias 16\n",
      "model_enc_blks.1.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.1.bias 16\n",
      "model_enc_blks.1.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.2.bias 16\n",
      "model_enc_blks.1.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.3.bias 16\n",
      "model_enc_blks.1.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.4.bias 16\n",
      "model_enc_blks.1.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.5.bias 16\n",
      "model_enc_blks.1.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.6.bias 16\n",
      "model_enc_blks.1.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wks.7.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.1.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.1.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.1.fc.weight 16384\n",
      "model_enc_blks.1.fc.bias 128\n",
      "model_enc_blks.1.norm_1.weight 64000\n",
      "model_enc_blks.1.norm_1.bias 64000\n",
      "model_enc_blks.1.norm_2.0.weight 64000\n",
      "model_enc_blks.1.norm_2.0.bias 64000\n",
      "model_enc_blks.1.norm_2.1.weight 64000\n",
      "model_enc_blks.1.norm_2.1.bias 64000\n",
      "model_enc_blks.1.norm_3.weight 64000\n",
      "model_enc_blks.1.norm_3.bias 64000\n",
      "model_enc_blks.2.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.2.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.2.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.2.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.2.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.2.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.2.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.2.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.2.self_att.W0.weight 16384\n",
      "model_enc_blks.2.self_att.W0.bias 128\n",
      "model_enc_blks.2.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.2.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.2.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.0.bias 16\n",
      "model_enc_blks.2.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.1.bias 16\n",
      "model_enc_blks.2.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.2.bias 16\n",
      "model_enc_blks.2.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.3.bias 16\n",
      "model_enc_blks.2.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.4.bias 16\n",
      "model_enc_blks.2.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.5.bias 16\n",
      "model_enc_blks.2.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.6.bias 16\n",
      "model_enc_blks.2.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wks.7.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.2.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.2.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.2.fc.weight 16384\n",
      "model_enc_blks.2.fc.bias 128\n",
      "model_enc_blks.2.norm_1.weight 64000\n",
      "model_enc_blks.2.norm_1.bias 64000\n",
      "model_enc_blks.2.norm_2.0.weight 64000\n",
      "model_enc_blks.2.norm_2.0.bias 64000\n",
      "model_enc_blks.2.norm_2.1.weight 64000\n",
      "model_enc_blks.2.norm_2.1.bias 64000\n",
      "model_enc_blks.2.norm_3.weight 64000\n",
      "model_enc_blks.2.norm_3.bias 64000\n",
      "model_enc_blks.3.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.3.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.3.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.3.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.3.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.3.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.3.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.3.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.3.self_att.W0.weight 16384\n",
      "model_enc_blks.3.self_att.W0.bias 128\n",
      "model_enc_blks.3.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.3.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.3.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.0.bias 16\n",
      "model_enc_blks.3.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.1.bias 16\n",
      "model_enc_blks.3.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.2.bias 16\n",
      "model_enc_blks.3.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.3.bias 16\n",
      "model_enc_blks.3.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.4.bias 16\n",
      "model_enc_blks.3.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.5.bias 16\n",
      "model_enc_blks.3.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.6.bias 16\n",
      "model_enc_blks.3.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wks.7.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.3.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.3.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.3.fc.weight 16384\n",
      "model_enc_blks.3.fc.bias 128\n",
      "model_enc_blks.3.norm_1.weight 64000\n",
      "model_enc_blks.3.norm_1.bias 64000\n",
      "model_enc_blks.3.norm_2.0.weight 64000\n",
      "model_enc_blks.3.norm_2.0.bias 64000\n",
      "model_enc_blks.3.norm_2.1.weight 64000\n",
      "model_enc_blks.3.norm_2.1.bias 64000\n",
      "model_enc_blks.3.norm_3.weight 64000\n",
      "model_enc_blks.3.norm_3.bias 64000\n",
      "model_enc_blks.4.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.4.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.4.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.4.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.4.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.4.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.4.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.4.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.4.self_att.W0.weight 16384\n",
      "model_enc_blks.4.self_att.W0.bias 128\n",
      "model_enc_blks.4.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.4.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.4.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.0.bias 16\n",
      "model_enc_blks.4.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.1.bias 16\n",
      "model_enc_blks.4.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.2.bias 16\n",
      "model_enc_blks.4.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.3.bias 16\n",
      "model_enc_blks.4.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.4.bias 16\n",
      "model_enc_blks.4.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.5.bias 16\n",
      "model_enc_blks.4.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.6.bias 16\n",
      "model_enc_blks.4.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wks.7.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.4.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.4.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.4.fc.weight 16384\n",
      "model_enc_blks.4.fc.bias 128\n",
      "model_enc_blks.4.norm_1.weight 64000\n",
      "model_enc_blks.4.norm_1.bias 64000\n",
      "model_enc_blks.4.norm_2.0.weight 64000\n",
      "model_enc_blks.4.norm_2.0.bias 64000\n",
      "model_enc_blks.4.norm_2.1.weight 64000\n",
      "model_enc_blks.4.norm_2.1.bias 64000\n",
      "model_enc_blks.4.norm_3.weight 64000\n",
      "model_enc_blks.4.norm_3.bias 64000\n",
      "model_enc_blks.5.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.5.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.5.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.5.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.5.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.5.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.5.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.5.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.5.self_att.W0.weight 16384\n",
      "model_enc_blks.5.self_att.W0.bias 128\n",
      "model_enc_blks.5.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.5.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.5.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.0.bias 16\n",
      "model_enc_blks.5.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.1.bias 16\n",
      "model_enc_blks.5.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.2.bias 16\n",
      "model_enc_blks.5.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.3.bias 16\n",
      "model_enc_blks.5.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.4.bias 16\n",
      "model_enc_blks.5.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.5.bias 16\n",
      "model_enc_blks.5.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.6.bias 16\n",
      "model_enc_blks.5.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wks.7.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.5.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.5.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.5.fc.weight 16384\n",
      "model_enc_blks.5.fc.bias 128\n",
      "model_enc_blks.5.norm_1.weight 64000\n",
      "model_enc_blks.5.norm_1.bias 64000\n",
      "model_enc_blks.5.norm_2.0.weight 64000\n",
      "model_enc_blks.5.norm_2.0.bias 64000\n",
      "model_enc_blks.5.norm_2.1.weight 64000\n",
      "model_enc_blks.5.norm_2.1.bias 64000\n",
      "model_enc_blks.5.norm_3.weight 64000\n",
      "model_enc_blks.5.norm_3.bias 64000\n",
      "model_enc_blks.6.convs.0.depthwise_conv.weight 640\n",
      "model_enc_blks.6.convs.0.depthwise_conv.bias 128\n",
      "model_enc_blks.6.convs.0.pointwise_conv.weight 16384\n",
      "model_enc_blks.6.convs.0.pointwise_conv.bias 128\n",
      "model_enc_blks.6.convs.1.depthwise_conv.weight 640\n",
      "model_enc_blks.6.convs.1.depthwise_conv.bias 128\n",
      "model_enc_blks.6.convs.1.pointwise_conv.weight 16384\n",
      "model_enc_blks.6.convs.1.pointwise_conv.bias 128\n",
      "model_enc_blks.6.self_att.W0.weight 16384\n",
      "model_enc_blks.6.self_att.W0.bias 128\n",
      "model_enc_blks.6.self_att.Wqs.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.0.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.1.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.2.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.3.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.4.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.5.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.6.bias 16\n",
      "model_enc_blks.6.self_att.Wqs.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wqs.7.bias 16\n",
      "model_enc_blks.6.self_att.Wks.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.0.bias 16\n",
      "model_enc_blks.6.self_att.Wks.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.1.bias 16\n",
      "model_enc_blks.6.self_att.Wks.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.2.bias 16\n",
      "model_enc_blks.6.self_att.Wks.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.3.bias 16\n",
      "model_enc_blks.6.self_att.Wks.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.4.bias 16\n",
      "model_enc_blks.6.self_att.Wks.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.5.bias 16\n",
      "model_enc_blks.6.self_att.Wks.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.6.bias 16\n",
      "model_enc_blks.6.self_att.Wks.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wks.7.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.0.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.0.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.1.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.1.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.2.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.2.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.3.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.3.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.4.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.4.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.5.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.5.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.6.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.6.bias 16\n",
      "model_enc_blks.6.self_att.Wvs.7.weight 2048\n",
      "model_enc_blks.6.self_att.Wvs.7.bias 16\n",
      "model_enc_blks.6.fc.weight 16384\n",
      "model_enc_blks.6.fc.bias 128\n",
      "model_enc_blks.6.norm_1.weight 64000\n",
      "model_enc_blks.6.norm_1.bias 64000\n",
      "model_enc_blks.6.norm_2.0.weight 64000\n",
      "model_enc_blks.6.norm_2.0.bias 64000\n",
      "model_enc_blks.6.norm_2.1.weight 64000\n",
      "model_enc_blks.6.norm_2.1.bias 64000\n",
      "model_enc_blks.6.norm_3.weight 64000\n",
      "model_enc_blks.6.norm_3.bias 64000\n",
      "pointer.w1.weight 256\n",
      "pointer.w1.bias 1\n",
      "pointer.w2.weight 256\n",
      "pointer.w2.bias 1\n"
     ]
    }
   ],
   "source": [
    "for name, p in a.named_parameters():\n",
    "    print(name, p.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
